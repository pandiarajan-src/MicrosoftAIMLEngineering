{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15dce04",
   "metadata": {},
   "source": [
    "# Fetch a Document Using a Python Web Scraper\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you've set up a basic web scraper using Python, we'll extend that knowledge to fetch a specific document, such as a PDF file or a text file, from a website. This is a common requirement when dealing with web scraping, as many useful data sources are available in downloadable documents. You will learn how to identify and download documents using the web scraper you built.\n",
    "\n",
    "By the end of this reading, you will be able to: \n",
    "\n",
    "Identify the location of document links within a webpage's HTML structure.\n",
    "\n",
    "Convert relative URLs to full URLs for document downloads.\n",
    "\n",
    "Download single and multiple documents effectively using Python.\n",
    "\n",
    "Implement best practices for responsible web scraping.\n",
    "\n",
    "### Step-by-step guide\n",
    "\n",
    "#### 1. Identify the document link\n",
    "\n",
    "Start by selecting a website that offers downloadable reports, research papers, or datasets. Look for sections labeled \"Downloads,\" \"Reports,\" or \"Resources.\" Before you can download a document, you need to identify its location on the webpage. Typically, documents are linked via <> (anchor) tags, which you can locate by inspecting the webpage's HTML structure. Right-click a document link and select Inspect to view its HTML. Identify an <> tag with an href attribute containing .pdf, .txt, or another document format. If the link is relative (e.g., /files/report.pdf), combine it with the base website URL to form a full URL before using it in your scraper.\n",
    "\n",
    "Example HTML snippet:\n",
    "<a href=\"/files/report.pdf\" class=\"download-link\">Download Report</a>\n",
    "\n",
    "In this example, the document (report.pdf) is linked within an anchor tag with the class \"download-link.\" The href attribute contains the path to the document.\n",
    "\n",
    "Parse the HTML and find the document link\n",
    "\n",
    "You can use BeautifulSoup to find the link to the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cbb185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Pandi's Chemistry App\n",
      "Document link found: pdf/periodic_table_explained.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Explanation:\n",
    "     This code sends a request to the webpage and parses the HTML. \n",
    "     It then searches for an <a> tag and extracts the href attribute, which contains the path to the document.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Step 1: Send an HTTP request to the webpage\n",
    "url = 'https://pandiarajan-src.github.io/Pandi-PeriodicTable/'  # Replace with the actual URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "else:\n",
    "    print('Failed to retrieve the webpage.')\n",
    "    exit()\n",
    "\n",
    "# Step 2: Find the document link\n",
    "document_link = soup.find('a')['href']\n",
    "\n",
    "# Print the document link to verify\n",
    "print('Title:', soup.title.text)\n",
    "print('Document link found:', document_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a91bbfc",
   "metadata": {},
   "source": [
    "#### 2. Handle relative URLs\n",
    "\n",
    "The link extracted from the webpage might be a relative URL (e.g., /files/report.pdf) rather than a full URL. You need to convert this into a full URL before making a request to download the document.\n",
    "\n",
    "**Convert the relative URL to a full URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dec3207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full URL: https://pandiarajan-src.github.io/Pandi-PeriodicTable/pdf/periodic_table_explained.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Explanation: \n",
    "    The os.path.join() function combines the base URL with the relative URL to form a full URL that can be used to download the document.\n",
    "\"\"\"\n",
    "# Handle relative URLs\n",
    "base_url = 'https://pandiarajan-src.github.io/Pandi-PeriodicTable/'  # The base URL of the website\n",
    "full_url = os.path.join(base_url, document_link)\n",
    "\n",
    "print('Full URL:', full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b7c3b",
   "metadata": {},
   "source": [
    "#### 3. Download the document\n",
    "\n",
    "With the full URL in hand, you can now send a request to download the document. The downloaded file can be saved to your local machine.\n",
    "\n",
    "**Download and save the document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ab2cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Explanation: \n",
    "    This code sends a GET request to the document's full URL. \n",
    "    If the request is successful, it writes the content of the response to a file.\n",
    "    The wb mode is used to write the file in binary format, which is necessary for non-text files such as PDFs.\n",
    "\"\"\"\n",
    "# Step 3: Download the document\n",
    "document_response = requests.get(full_url)\n",
    "\n",
    "# Check if the document request was successful\n",
    "if document_response.status_code == 200:\n",
    "    # Save the document to a file\n",
    "    with open('report.pdf', 'wb') as file:\n",
    "        file.write(document_response.content)\n",
    "    print('Document downloaded successfully.')\n",
    "else:\n",
    "    print('Failed to download the document. Status code:', document_response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609090ad",
   "metadata": {},
   "source": [
    "#### 4. Fetch multiple documents\n",
    "\n",
    "If a webpage contains multiple documents you want to download, you can modify the scraper to loop through all the document links and download each one.\n",
    "\n",
    "**Example: Downloading multiple documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e112b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 downloaded successfully as report_1.pdf.\n",
      "Document 2 downloaded successfully as report_2.pdf.\n",
      "Failed to download document 3. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Explanation: \n",
    "    This code snippet finds all the document links on the page, \n",
    "    iterates through them, and downloads each document, \n",
    "    saving it with a unique filename.\n",
    "\"\"\"\n",
    "# Find all document links on the page\n",
    "document_links = soup.find_all('a')\n",
    "\n",
    "# Loop through each link and download the corresponding document\n",
    "for i, link in enumerate(document_links):\n",
    "    document_url = os.path.join(base_url, link['href'])\n",
    "    document_response = requests.get(document_url)\n",
    "    \n",
    "    if document_response.status_code == 200:\n",
    "        # Save each document with a unique name\n",
    "        file_name = f'report_{i+1}.pdf'\n",
    "        with open(file_name, 'wb') as file:\n",
    "            file.write(document_response.content)\n",
    "        print(f'Document {i+1} downloaded successfully as {file_name}.')\n",
    "    else:\n",
    "        print(f'Failed to download document {i+1}. Status code:', document_response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf634be",
   "metadata": {},
   "source": [
    "#### 5. Important considerations\n",
    "\n",
    " - **Respect website permissions:** Always check the website’s terms of service to ensure you’re allowed to download documents using automated tools.\n",
    " - **Handle different file types accordingly**: Depending on the type of document, you might need to adjust your code to handle different file formats (e.g., .txt, .csv, and .docx).\n",
    " - **Manage large downloads**: If you’re downloading large files, consider adding error handling and resuming capabilities to your scraper.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By extending your web scraper to fetch documents, you can automate the process of acquiring valuable resources from the web. Whether you’re downloading reports, datasets, or other types of files, understanding how to identify document links and handle file downloads is a critical skill in data acquisition. With the examples provided, you should be well equipped to implement document fetching in your web scraping projects.\n",
    "\n",
    "Continue experimenting with different websites and document types to refine your scraper, and always remember to scrape responsibly and ethically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
