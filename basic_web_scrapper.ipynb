{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f78a58",
   "metadata": {},
   "source": [
    "# Practice Activity: Setup of a Basic Data Scraper in Python\n",
    "## Introduction\n",
    "\n",
    "Web scraping is a powerful method for acquiring data from websites, especially when the information you need isn’t readily available in a structured format. By setting up a web scraper in your local environment, you can automate the process of gathering large amounts of data from the web. \n",
    "\n",
    "By the end of this reading, you will be able to: \n",
    "\n",
    "Set up a basic data scraper using Python, including code snippets and explanations to help you get started.\n",
    "\n",
    "### 1. Prerequisites\n",
    "\n",
    "Before diving into the code, ensure you have the following tools installed on your local environment:\n",
    "\n",
    "- **Python 3.x**: Python is the language we’ll use to build our web scraper.\n",
    "- **pip**: pip is Python’s package installer, which you’ll use to install the  necessary libraries.\n",
    "- **A code editor**: Examples include Jupyter Notebooks, VS Code, PyCharm, or even a simple text editor such as Sublime Text.\n",
    "\n",
    "You’ll also need a basic understanding of HTML, as web scraping involves interacting with the HTML structure of a web page.\n",
    "\n",
    "### 2. Writing the Python script\n",
    "\n",
    "Now, let’s walk through the code to set up a basic web scraper that extracts data from a web page.\n",
    "\n",
    "Step-by-step guide:\n",
    "\n",
    "#### Step 1: Import the necessary libraries\n",
    "Start by importing the libraries you’ll need, if they’re not already in your kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69a8cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f6aeb",
   "metadata": {},
   "source": [
    "#### Step 2: Send an HTTP request to the website\n",
    "\n",
    "Use the requests library to send an HTTP GET request to the website you want to scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a89b9369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n"
     ]
    }
   ],
   "source": [
    "# url = 'https://example.com'  # Replace with the URL of the website you want to scrape\n",
    "url = 'https://pandiarajan-src.github.io/Pandi-PeriodicTable/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print('Request successful!')\n",
    "else:\n",
    "    print('Failed to retrieve the webpage')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70649fc1",
   "metadata": {},
   "source": [
    "#### Step 3: Parse the HTML content\n",
    "\n",
    "Once you’ve successfully retrieved the web page, use BeautifulSoup to parse the HTML content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf4682af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandi's Chemistry App\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Print the title of the webpage to verify\n",
    "print(soup.title.text)\n",
    "#print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50078795",
   "metadata": {},
   "source": [
    "#### Step 4: Extract the data you need\n",
    "\n",
    "Now that you have the HTML parsed, you can start extracting the data you’re interested in. Let’s say you want to scrape a list of items from a table on the web page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de8d94ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         0                                                  1\n",
      "0                     None                                               None\n",
      "1            Alkali Metals  Highly reactive metals in Group 1 of the perio...\n",
      "2    Alkaline Earth Metals  Reactive metals in Group 2 of the periodic table.\n",
      "3        Transition Metals  Metals with variable oxidation states, found i...\n",
      "4   Post Transition Metals  Metals with lower melting points and softer th...\n",
      "5               Metalloids  Elements with properties of both metals and no...\n",
      "6                Nonmetals  Elements that are poor conductors of heat and ...\n",
      "7                 Halogens  Highly reactive nonmetals in Group 17 of the p...\n",
      "8              Noble Gases     Inert gases in Group 18 of the periodic table.\n",
      "9              Lanthanides   Rare earth metals, part of the f-block elements.\n",
      "10               Actinides  Radioactive elements, part of the f-block elem...\n"
     ]
    }
   ],
   "source": [
    "# Find the table containing the data\n",
    "table = soup.find('table', {'id': 'element-categories'})  # Replace 'data-table' with the actual id or class of the table\n",
    "\n",
    "# Extract table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract headers from the first row (using <th> tags)\n",
    "# headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
    "\n",
    "# Loop through the rows and extract data (skip the first row with headers)\n",
    "data = []\n",
    "for row in rows[0:]:  # Start from the second row onwards\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    \n",
    "    # Skip rows with mismatched column counts\n",
    "    data.append(cols)\n",
    "    # if len(cols) == len(headers):\n",
    "    #     data.append(cols)\n",
    "\n",
    "# Convert the data into a pandas DataFrame, using the extracted headers as column names\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the scraped data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2646e",
   "metadata": {},
   "source": [
    "#### Step 5: Save the scraped data\n",
    "\n",
    "Finally, you can save the scraped data to a file for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f99d764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990b3c4",
   "metadata": {},
   "source": [
    "### 3. Example: Scraping information from Wikipedia\n",
    "\n",
    "Let’s go through a more concrete example where we scrape information about cloud computing platforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb2dfaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Cloud-computing comparison - Wikipedia\n",
      "                      Provider Launched Block storage Assignable IPs  \\\n",
      "0        Google Cloud Platform     2013           Yes             No   \n",
      "1  Oracle Cloud Infrastructure     2014           Yes            Yes   \n",
      "2          Amazon Web Services     2006           Yes            Yes   \n",
      "3                    IBM Cloud     2005           Yes            Yes   \n",
      "4              Microsoft Azure     2010           Yes            Yes   \n",
      "\n",
      "  SMTP support IOPS Guaranteed minimum Security  \\\n",
      "0        No[1]                     Yes   Yes[2]   \n",
      "1          Yes                     Yes   Yes[5]   \n",
      "2   Partial[6]                     Yes   Yes[7]   \n",
      "3        No[9]                     Yes  Yes[10]   \n",
      "4      Yes[11]                     Yes  Yes[12]   \n",
      "\n",
      "                                           Locations             Notes  \n",
      "0  br, ca, cl, us, be, ch, de, es, fi, it, po, nl...  SMTP blocked.[4]  \n",
      "1  us, ca, br, de, uk, nl, ch, in, aus, jp, kr, saud                    \n",
      "2  us, ca, br, ie, de, uk, cn, sg, au, jp, kr, in...   List of bugs[8]  \n",
      "3  us, gb, fr, de, nl, in, au, hk, kr, it, jp, no...                    \n",
      "4  ca, us, br, ie, nl, de, uk, cn, au, jp, in, kr...  List of bugs[13]  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send an HTTP request to the webpage\n",
    "url = 'https://en.wikipedia.org/wiki/Cloud-computing_comparison'  \n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Print the title of the webpage to verify\n",
    "print(\"Title: \" + soup.title.text)\n",
    "\n",
    "# Find the table containing the data (selecting the first table by default)\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract headers from the first row (using <th> tags)\n",
    "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
    "\n",
    "# Loop through the rows and extract data (skip the first row with headers)\n",
    "data = []\n",
    "for row in rows[1:]:  # Start from the second row onwards\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "# Convert the data into a pandas DataFrame, using the extracted headers as column names\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(df.head())  \n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aba1e3",
   "metadata": {},
   "source": [
    "### 4. Important considerations\n",
    "\n",
    "**Respect the website’s terms of service:** Always check the website’s terms of service to ensure that you’re allowed to scrape i**ts content. Some websites explicitly prohibit scraping.\n",
    "\n",
    "**Be mindful of rate limits:** Avoid sending too many requests in a short period to prevent overloading the website’s server. Implement delays between requests if necessary.\n",
    "\n",
    "**Handle errors gracefully:** Always include error handling in your script to manage situations where the website structure changes or the page fails to load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5da5cd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By setting up a basic web scraper in Python, you can automate the process of gathering data from websites, making it easier to acquire the information you need for your AI/ML projects. \n",
    "\n",
    "You've just learned the fundamentals of web scraping, from sending HTTP requests and parsing HTML to extracting and saving data. With this foundation, you can expand your scraper to handle more complex scenarios and integrate the data into your ML models.\n",
    "\n",
    "Continue practicing with different websites and data structures, and always remember to scrape responsibly and ethically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
